# -*- coding: utf-8 -*-
"""lvadsusr92_kamalakannan_IA2 Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fBICHnITM6PuaRvD__2OYNQUz173yZ4g
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("Mall_Customers.csv")

df.isnull().sum()

numerical_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_cols = ['Gender']

numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

data_preprocessed = pd.DataFrame(preprocessor.fit_transform(df))

data_preprocessed.isnull().sum()

scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_preprocessed)

sns.pairplot(data_preprocessed)
plt.show()

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(data_scaled)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
cluster_labels = kmeans.fit_predict(data_scaled)
df['Cluster'] = cluster_labels

cluster_profile = df.groupby('Cluster').mean()
print("\nCluster profile:")
print(cluster_profile)

# Feature engineering: Add new feature - Ratio of Spending to Income
data_preprocessed['Spending_Income_Ratio'] = data_preprocessed[2] / data_preprocessed[1]

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

data = pd.read_csv('Mall_Customers.csv')

print("Original dataset:")
print(data.head())

print("\nMissing values before imputation:")
print(data.isnull().sum())

# Separate numerical and categorical columns
numerical_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_cols = ['Gender']

# Preprocessing pipeline
numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply preprocessing
data_preprocessed = pd.DataFrame(preprocessor.fit_transform(data))

# Replace missing values with mean (only for numerical columns)
data_preprocessed.fillna(data_preprocessed.mean(), inplace=True)

# Data normalization
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_preprocessed)

# EDA statistics
print("\nEDA statistics:")
print(data_preprocessed.describe())

# EDA visualization
# Pairplot of selected features
sns.pairplot(data_preprocessed)
plt.show()

# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(data_scaled)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using silhouette score
silhouette_scores = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(data_scaled)
    silhouette_avg = silhouette_score(data_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)
optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2
print("Optimal number of clusters (Silhouette score):", optimal_n_clusters)

# Apply k-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, init='k-means++', random_state=42)
cluster_labels = kmeans.fit_predict(data_scaled)
data['Cluster'] = cluster_labels

# Cluster profiling
cluster_profile = data.groupby('Cluster').mean()
print("\nCluster profile:")
print(cluster_profile)

# Feature engineering: Add new feature - Ratio of Spending to Income
data_preprocessed['Spending_Income_Ratio'] = data_preprocessed[2] / data_preprocessed[1]